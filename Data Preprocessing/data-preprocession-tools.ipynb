{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Numpy is used to perform operations and manipulate arrays\n",
    "import matplotlib.pyplot as plt # Matplotlib is used to plot graphs and various visual charts\n",
    "import pandas as pd # Pandas is used as a data preprocessor, used for importing datasets such as csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv') # Reads the data from the csv file and creates a dataframe which we can manipulate\n",
    "'''\n",
    "X is the independent variable, which is used to \n",
    "predict the value of the independent variable. \n",
    "Here the dependent variables are country, age and salary.\n",
    "'''\n",
    "X = dataset.iloc[:, :-1].values # Selecting the country, age and salary columns\n",
    "# iloc is used to locate the particular row and column. \n",
    "# First parameter is the row and 2nd parameter is the column\n",
    "'''\n",
    "Y is the dependent variable, which is predicted\n",
    "based on the value of X. Here the independent variable\n",
    "is purchased.\n",
    "'''\n",
    "Y = dataset.iloc[:, -1].values # Selecting the purchased column\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care of mising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the SimpleImputer class from the scikit learn library\n",
    "Impute means to simply assign something a new value\n",
    "Here, the missing values of the columns are replaced with the average values of the column\n",
    "i.e the missing values are imputed with the new value which is the mean column value\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Creating a SimpleImputer object, missing values are identified as nan and mean column value is used to replace it\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") \n",
    "# Fitting the data to the constraints specified and transforming it. fit_transform \n",
    "# creates a copy of the data specified and returns a new matrix\n",
    "X[:, 1:] = imputer.fit_transform(X[:, 1:])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data\n",
    "\n",
    "Categorical data is basically data which is not numerical. This data is encoded into numbers in order to manipulate/work in a convenient manner.\n",
    "\n",
    "One hot encoding technique is used in this example, which means that every different value is transformed into a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer # ColumnTransformer is used to transform the exiting columns into the ones specified\n",
    "\n",
    "# OneHotEncoder is used to encode the columns into numerical columns\n",
    "# Here France is given the code 100, Spain is given the code 010 and Germany the code 001,\n",
    "# Where each number represents a separate column\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# Takes two arguments transformers and remainder\n",
    "# Transformer is a tuple consisting of three things, the type of transformation, transformer used and the columns to transform\n",
    "# Remainder is either drop or passthrough, i.e drop the other columns or retain them\n",
    "ColumnTransformerObj = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "X = np.array(ColumnTransformerObj.fit_transform(X))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder is used to encode every value(numerical or categorical) specified into a unique numerical value.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LabelEncoderObj = LabelEncoder()\n",
    "Y = LabelEncoderObj.fit_transform(Y)\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training set and test set\n",
    "\n",
    "The dataset used is splitted into two halves namely training set and test set. Training set is used to train the model and test set is used to test the accuracy of the model. Generally 80% of the data is used to train the model and 20% to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_Train)\n",
    "print(X_Test)\n",
    "\n",
    "print(Y_Train)\n",
    "print(Y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "It is used to put all the features on the same scale, such that no feature dominates the other one. Feature scaling should happen before testing as to prevent any information leakage to the test set.\n",
    "\n",
    "The main features scaling techniques are : \n",
    "1) Standardization\n",
    "2) Normalization\n",
    "\n",
    "Standardization: \n",
    "\n",
    "By applying standardization technique, we get a value between +3 and -3. It can be used for any kind of data. It is not applied for dummy data as it already lies between -3 and +3.\n",
    "\n",
    "<img src=\"http://www.sciweavers.org/upload/Tex2Img_1642227428/render.png\">\n",
    "\n",
    "Normalization:\n",
    "\n",
    "By applying normalization technique, we get a value between 0 and 1. It is recommended to use when dealing with data following normal distribution.\n",
    "\n",
    "<img src=\"http://www.sciweavers.org/upload/Tex2Img_1642227737/render.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # Applies the standardization technique\n",
    "StandardScalerObj = StandardScaler()\n",
    "\n",
    "# Calcuates the mean and standard deviation \n",
    "# and transforms the values using standardization techniques\n",
    "X_Train[:, 3:] = StandardScalerObj.fit_transform(X_Train[:, 3:])\n",
    "\n",
    "# Transforms the test set based on the mean and sd of the training set\n",
    "X_Test[:, 3:] = StandardScalerObj.transform(X_Test[:, 3:])\n",
    "\n",
    "print(X_Train)\n",
    "print(X_Test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0dd272924c7cc597c5889018966c2ccb6c9651b4aec44649117dbf66b8c76eee"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
